{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cbaf8c-4c0b-46b4-973b-cd5207e95ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6513473-a657-4900-a87c-80f4016a0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vcf \n",
    "from Bio import SeqIO\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def making_tokens(seq):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t')\n",
    "    tokens = tokenizer.tokenize(seq,truncation=True,max_length=512)\n",
    "    encoded_sequence = tokenizer.encode_plus(sequence,return_offsets_mapping=True,truncation=True,max_length=512)\n",
    "    offsets = encoded_sequence[\"offset_mapping\"]\n",
    "    print(tokens)\n",
    "    token_ids = np.array(encoded_sequence[\"input_ids\"])\n",
    "    tokens_coord={}\n",
    "    for i in range(1, len(token_ids) - 1):\n",
    "        start_char = offsets[i][0]\n",
    "        end_char = offsets[i][1]\n",
    "        token = tokenizer.decode([token_ids[i]])\n",
    "        tokens_coord[token]=(start_char,end_char)\n",
    "        \n",
    "    return tokens_coord\n",
    "\n",
    "\n",
    "\n",
    "def changing_tokens_via_vcf(path_to_VCF, chrom, seq, seq_start, seq_end, dict_with_tokens_and_coord):\n",
    "    reader = vcf.Reader(filename=path_to_VCF)\n",
    "    reader.fetch(chrom,start=seq_start,end=seq_end)\n",
    "    tokens_array=dict_with_tokens_and_coord.keys()\n",
    "    tokens_coord=dict_with_tokens_and_coord.values()\n",
    "    tokens_starts = [item[0] + seq_start for item in tokens_coord]\n",
    "    tokens_ends = [item[1] + seq_start for item in tokens_coord]\n",
    "    tokens_data=pd.DataFrame({'tokens':tokens_array,'start':tokens_starts,'end':tokens_ends})\n",
    "    count_total_record_in_tokens=0\n",
    "    new_unique_tokens=0\n",
    "    for record in reader:\n",
    "        record_in_token=tokens_data[(tokens_data['start']<=record.start) & (tokens_data['end']>=record.end)]\n",
    "        if len(record_in_token)==1:\n",
    "            count_total_record_in_tokens+=1\n",
    "            start_in_token=record.start-record_in_token['start'].values + 1\n",
    "            end_in_token=record.end - record_in_token['start'].values + 1\n",
    "            new_token = record_in_token['tokens'].values\n",
    "            new_token = ''.join(new_token)\n",
    "            record_str=''.join(map(str, record.ALT))\n",
    "            new_token = new_token[:int(start_in_token)] + record_str + new_token[int(end_in_token):]\n",
    "            if new_token not in tokens_array:\n",
    "                new_unique_tokens+=1\n",
    "    return(new_unique_tokens*100/count_total_record_in_tokens)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "def get_sequence_from_fasta(fasta_file, chromosome, start, end):\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        if record.id == chromosome:\n",
    "            return record.seq[start-1:end] \n",
    "\n",
    "\n",
    "chrom_to_choose='chr1'\n",
    "vcf_path='/beegfs/data/hpcws/ws1/popov-popov_gnomad/GNOMAD/gnomad/gnomad.genomes.v4.0.sites.'+chrom_to_choose+'.vcf.gz'\n",
    "chromsizes_for_org=pd.read_csv('hg38.chrom.sizes',sep='\\t',header=None)\n",
    "len_of_chrom=chromsizes_for_org[chromsizes_for_org[0]==chrom_to_choose][1]\n",
    "start_random_coordinates=np.random.randint(0,int(len_of_chrom)-6000,size=500)\n",
    "end_random_coordinates=start_random_coordinates+6000\n",
    "prop_of_new_tokens=[]\n",
    "for i in tqdm.tqdm(range(len(start_random_coordinates))):\n",
    "    sequence=get_sequence_from_fasta('hg38.fa',chrom_to_choose, start_random_coordinates[i], end_random_coordinates[i]).upper()\n",
    "    sequence='NNNAGCTNNNNNNNNNNN'\n",
    "    dict_tokens=making_tokens(sequence)\n",
    "    prop_of_new_tokens.append(changing_tokens_via_vcf(vcf_path,chrom_to_choose, sequence, start_random_coordinates[i], end_random_coordinates[i],dict_tokens))\n",
    "data=pd.DataFrame({'prop_of_new_tokens':prop_of_new_tokens})\n",
    "data.to_csv('prop_of_new_tokens.txt',sep='\\t',index=False,header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7bb37e97-ab5d-40bd-896b-4741f0b36c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [02:23<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время на один токен:6.581866178536202e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vcf \n",
    "from Bio import SeqIO\n",
    "import tqdm\n",
    "from pysam import FastaFile\n",
    "import time \n",
    "\n",
    "def making_tokens(seq):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('AIRI-Institute/gena-lm-bert-base-t2t')\n",
    "    tokens = tokenizer.tokenize(seq)\n",
    "    encoded_sequence = tokenizer.encode_plus(sequence,return_offsets_mapping=True)\n",
    "    offsets = encoded_sequence[\"offset_mapping\"]\n",
    "    token_ids = np.array(encoded_sequence[\"input_ids\"])\n",
    "    tokens_coord={}\n",
    "    general_tokens_dict=tokenizer.vocab.keys()\n",
    "    for i in range(1, len(token_ids) - 1):\n",
    "        start_char = offsets[i][0]\n",
    "        end_char = offsets[i][1]\n",
    "        token = tokenizer.decode([token_ids[i]])\n",
    "        tokens_coord[(start_char,end_char)]=token\n",
    "    return tokens_coord,general_tokens_dict\n",
    "\n",
    "\n",
    "\n",
    "def changing_tokens_via_vcf(path_to_VCF, chrom, seq, seq_start, seq_end, dict_with_tokens_and_coord,general_tokens_dict):\n",
    "    reader = vcf.Reader(filename=path_to_VCF)\n",
    "    reader.fetch(chrom,start=seq_start,end=seq_end)\n",
    "    tokens_coord=dict_with_tokens_and_coord.keys()\n",
    "    tokens_array=dict_with_tokens_and_coord.values()\n",
    "    tokens_starts = [item[0] + seq_start for item in tokens_coord]\n",
    "    tokens_ends = [item[1] + seq_start for item in tokens_coord]\n",
    "    tokens_data=pd.DataFrame({'tokens':tokens_array,'start':tokens_starts,'end':tokens_ends})\n",
    "    count_total_record_in_tokens=0\n",
    "    new_unique_tokens=0\n",
    "    count_wrong=0\n",
    "    af_list=[]\n",
    "    time_list=[]\n",
    "    for record in reader:\n",
    "        record_in_token=tokens_data[(tokens_data['start']<=record.start) & (tokens_data['end']>=record.end)]\n",
    "        if len(record_in_token)==1:\n",
    "            start_time = time.time()\n",
    "            count_total_record_in_tokens+=1\n",
    "            start_in_token=record.start-record_in_token['start'].values\n",
    "            end_in_token=record.end - record_in_token['start'].values\n",
    "            new_token = record_in_token['tokens'].values\n",
    "            new_token = ''.join(new_token)\n",
    "            record_str=''.join(map(str, record.ALT))\n",
    "            new_token = new_token[:(start_in_token[0])] + record_str + new_token[(end_in_token[0]):]\n",
    "            af_list=record.INFO['AF']\n",
    "            time_per_token = time.time() - start_time\n",
    "            time_list.append(time_per_token)\n",
    "        else:\n",
    "            record_in_token=tokens_data[((tokens_data['start']<=record.start) & (tokens_data['end']<=record.end) & (record.start<tokens_data['end'])) | \n",
    "                                        ((tokens_data['start']>=record.start) & (tokens_data['start']-1<=record.end) & (record.end<=tokens_data['end']))].reset_index()\n",
    "            if len(record_in_token)!=1:\n",
    "                start_time = time.time()\n",
    "                count_total_record_in_tokens+=2\n",
    "                start_in_token=record.start-record_in_token.loc[0,'start']\n",
    "                new_token = record_in_token.loc[0,'tokens']\n",
    "                end_in_token=len(new_token)\n",
    "                new_token = ''.join(new_token)\n",
    "                record_str=''.join(map(str, record.ALT))\n",
    "                new_token_0 = new_token[:int(start_in_token)] + record_str\n",
    "                start_in_token=record.end - record_in_token.loc[1,'start']\n",
    "                new_token = record_in_token.loc[1,'tokens']\n",
    "                new_token_1 = new_token[int(start_in_token):]\n",
    "                time_per_token = (time.time() - start_time)/2\n",
    "                time_list.append(time_per_token)\n",
    "                af_list=record.INFO['AF']\n",
    "    return np.mean(time_list)  \n",
    "        \n",
    "genome='/data/aapopov/Projects/gnomad/gnomad_tokens_experiment/hg38.fa'\n",
    "sequences_object = FastaFile(genome)\n",
    "chrom_to_choose='chr14'\n",
    "vcf_path='/data/aapopov/Projects/gnomad/gnomad_tokens_experiment/DICER1_chr14_94987924_95184532.vcf.gz'\n",
    "chromsizes_for_org=pd.read_csv('hg38.chrom.sizes',sep='\\t',header=None)\n",
    "len_of_chrom=chromsizes_for_org[chromsizes_for_org[0]==chrom_to_choose][1]\n",
    "start_random_coordinates=np.random.randint(94987924,95184532-6000,size=100)\n",
    "end_random_coordinates=start_random_coordinates+6000\n",
    "prop_of_new_tokens=[]\n",
    "for i in tqdm.tqdm(range(len(start_random_coordinates))):\n",
    "    sequence=sequences_object.fetch(chrom_to_choose, start_random_coordinates[i], end_random_coordinates[i]).upper()\n",
    "    dict_tokens,general_tokens_dict=making_tokens(sequence)\n",
    "    prop_of_new_tokens.append(changing_tokens_via_vcf(vcf_path,chrom_to_choose, sequence, start_random_coordinates[i], end_random_coordinates[i],dict_tokens,general_tokens_dict))\n",
    "print(f'Среднее время на один токен:{np.mean(prop_of_new_tokens)}')\n",
    "data=pd.DataFrame({'prop_of_new_tokens':prop_of_new_tokens})\n",
    "data.to_csv('prop_of_new_tokens.txt',sep='\\t',index=False,header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41666a21-2ac6-45ea-a58a-a89f3c1fda6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5febad-1125-443e-8d30-f0bb1e52654b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d34650-bd60-42cd-84f0-43e430492de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
